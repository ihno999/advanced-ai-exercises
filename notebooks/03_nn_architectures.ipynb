{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Dive into Neural Network Architectures\n",
    "\n",
    "In this lecture, we'll explore different types of neural networks that have revolutionized various fields, including image recognition, natural language processing, and generative modeling. We'll cover Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Long Short-Term Memory networks (LSTMs). Get ready for a hands-on workshop experience with PyTorch!\n",
    "\n",
    "**Prerequisites:** Basic understanding of neural networks, activation functions, backpropagation, and training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNNs)\n",
    "\n",
    "CNNs are designed for processing grid-like data, such as images. They excel at capturing spatial hierarchies and local patterns, making them ideal for tasks like image recognition, object detection, and image segmentation.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "*   **Convolutional Layers:** These layers apply convolutional kernels (filters) to the input, extracting features like edges, corners, and textures.  \n",
    "    [Image of convolutional layer operation]\n",
    "*   **Pooling Layers:** Pooling layers downsample the feature maps, reducing dimensionality and making the network more robust to variations in the input.  \n",
    "    [Image of pooling layer operation (max pooling)]\n",
    "*   **Activation Functions:**  Non-linear activation functions (like ReLU) introduce non-linearity, enabling the network to learn complex patterns.\n",
    "*   **Fully Connected Layers:** These layers connect all neurons in one layer to all neurons in the next layer, typically used for final classification or regression.\n",
    "\n",
    "### Practical Example: Image Classification with CNNs\n",
    "\n",
    "Let's build a CNN to classify images from the CIFAR-10 dataset using PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "model = CNN()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop (simplified for brevity)\n",
    "for epoch in range(2):  # Adjust the number of epochs as needed\n",
    "    for i, (inputs, labels) in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch: {epoch + 1}, Batch: {i}, Loss: {loss.item()}\")\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs)\n",
    "\n",
    "RNNs are designed to handle sequential data by incorporating recurrent connections, allowing them to maintain information about previous inputs. This makes them suitable for tasks like natural language processing, time series analysis, and speech recognition.\n",
    "\n",
    "### Key Idea\n",
    "\n",
    "RNNs have a hidden state that is updated at each time step based on the current input and the previous hidden state. This hidden state acts as a memory, capturing information about the sequence seen so far.  \n",
    "[Image of a simple RNN cell with recurrent connection]\n",
    "\n",
    "### Challenges\n",
    "\n",
    "*   **Vanishing Gradients:**  RNNs can suffer from vanishing gradients, making it difficult to learn long-term dependencies in sequences.\n",
    "\n",
    "### Practical Example: Text Generation with RNNs\n",
    "\n",
    "Let's build a simple RNN to generate text character by character using PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Sample text data\n",
    "text = \"\"\"This is an example text to train our RNN model. \n",
    "We will feed this text to the network and see how it learns \n",
    "to generate new text character by character.\"\"\"\n",
    "\n",
    "# Create a character-level vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Create a custom dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, seq_length):\n",
    "        self.text = text\n",
    "        self.seq_length = seq_length\n",
    "        self.data =\n",
    "        for i in range(0, len(text) - seq_length, 1):\n",
    "            seq_in = text[i:i + seq_length]\n",
    "            seq_out = text[i + seq_length]\n",
    "            self.data.append((seq_in, seq_out))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq_in, seq_out = self.data[index]\n",
    "        input_seq = torch.tensor([char_to_idx[ch] for ch in seq_in], dtype=torch.long)\n",
    "        output_seq = torch.tensor(char_to_idx[seq_out], dtype=torch.long)\n",
    "        return input_seq, output_seq\n",
    "\n",
    "# Hyperparameters\n",
    "seq_length = 30\n",
    "hidden_size = 128\n",
    "learning_rate = 0.01\n",
    "epochs = 50\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TextDataset(text, seq_length)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Define the RNN model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "        x = self.fc(x)\n",
    "        return x, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "model = RNN(vocab_size, hidden_size, vocab_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop (simplified for brevity)\n",
    "for epoch in range(epochs):\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        hidden = model.init_hidden()\n",
    "        optimizer.zero_grad()\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch: {epoch + 1}, Batch: {i}, Loss: {loss.item()}\")\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory Networks (LSTMs)\n",
    "\n",
    "LSTMs are a type of RNN designed to address the vanishing gradient problem and capture long-term dependencies in sequences. They achieve this through a more complex cell structure with gates that control the flow of information.\n",
    "\n",
    "### Key Idea\n",
    "\n",
    "LSTMs introduce memory cells and gates (input, forget, output) that regulate the flow of information into and out of the cell. This allows them to selectively remember or forget information, enabling them to learn long-term dependencies.  \n",
    "[Image of an LSTM cell with gates]\n",
    "\n",
    "### Practical Example: Time Series Prediction with LSTMs\n",
    "\n",
    "Let's build an LSTM to predict future values in a time series using PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic time series data\n",
    "data = np.sin(np.linspace(0, 20, 1000)) + 0.2 * np.random.randn(1000)\n",
    "\n",
    "# Normalize data\n",
    "data = (data - np.mean(data)) / np.std(data)\n",
    "\n",
    "# Create sequences\n",
    "seq_length = 50\n",
    "X =\n",
    "y =\n",
    "for i in range(len(data) - seq_length - 1):\n",
    "    X.append(data[i:i + seq_length])\n",
    "    y.append(data[i + seq_length])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(2)  # Add a dimension for the feature\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).unsqueeze(2)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out[-1])  # Take the output from the last time step\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size),\n",
    "                torch.zeros(1, 1, self.hidden_size))\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "model = LSTM(1, hidden_size, 1)  # Input size is 1 (single feature)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop (simplified for brevity)\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(X_train)):\n",
    "        hidden = model.init_hidden()\n",
    "        optimizer.zero_grad()\n",
    "        outputs, hidden = model(X_train[i].unsqueeze(1), hidden)  # Add a dimension for the sequence\n",
    "        loss = criterion(outputs, y_train[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Prediction and visualization (simplified for brevity)\n",
    "with torch.no_grad():\n",
    "    predictions =\n",
    "    hidden = model.init_hidden()\n",
    "    for i in range(len(X_test)):\n",
    "        output, hidden = model(X_test[i].unsqueeze(1), hidden)\n",
    "        predictions.append(output.item())\n",
    "\n",
    "plt.plot(y_test, label='Actual')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks (GANs)\n",
    "\n",
    "GANs consist of two neural networks, a generator and a discriminator, that compete against each other in a unique training process. The generator tries to create realistic data samples, while the discriminator tries to distinguish between real and generated samples. This adversarial process pushes both networks to improve, leading to the generation of highly realistic data.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "*   **Generator:** Takes random noise as input and generates data samples.\n",
    "*   **Discriminator:** Takes data samples (real or generated) as input and tries to classify them as real or fake.\n",
    "*   **Adversarial Training:** The generator and discriminator are trained in tandem, with the generator trying to fool the discriminator and the discriminator trying to avoid being fooled.\n",
    "\n",
    "### Practical Example: Image Generation with GANs\n",
    "\n",
    "Let's build a simple GAN to generate images from the MNIST dataset using PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define the Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, image_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, image_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(image_size, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "image_size = 28 * 28\n",
    "learning_rate = 0.0002\n",
    "epochs = 200\n",
    "\n",
    "# Initialize Generator and Discriminator\n",
    "generator = Generator(latent_dim, image_size)\n",
    "discriminator = Discriminator(image_size)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Training loop (simplified for brevity)\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, _) in enumerate(trainloader):\n",
    "        # Adversarial ground truths\n",
    "        real_labels = torch.ones(images.size(0), 1)\n",
    "        fake_labels = torch.zeros(images.size(0), 1)\n",
    "\n",
    "        # Train Discriminator\n",
    "        discriminator.zero_grad()\n",
    "        real_images = images.view(images.size(0), -1)\n",
    "        real_outputs = discriminator(real_images)\n",
    "        d_loss_real = criterion(real_outputs, real_labels)\n",
    "        d_loss_real.backward()\n",
    "\n",
    "        noise = torch.randn(images.size(0), latent_dim)\n",
    "        fake_images = generator(noise)\n",
    "        fake_outputs = discriminator(fake_images.detach())\n",
    "        d_loss_fake = criterion(fake_outputs, fake_labels)\n",
    "        d_loss_fake.backward()\n",
    "\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        generator.zero_grad()\n",
    "        fake_outputs = discriminator(fake_images)\n",
    "        g_loss = criterion(fake_outputs, real_labels)  # Try to fool the discriminator\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            print(f\"Epoch: {epoch + 1}, Batch: {i}, D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")\n",
    "\n",
    "    # Generate and visualize some images (simplified for brevity)\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(64, latent_dim)\n",
    "        generated_images = generator(noise).view(64, 1, 28, 28)\n",
    "        for i in range(8):\n",
    "            for j in range(8):\n",
    "                plt.subplot(8, 8, i * 8 + j + 1)\n",
    "                plt.imshow(generated_images[i * 8 + j].squeeze(), cmap='gray')\n",
    "                plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this workshop, we've explored a variety of powerful neural network architectures:\n",
    "\n",
    "* CNNs: We saw how CNNs excel at processing images and extracting spatial hierarchies using convolutional and pooling layers.\n",
    "* RNNs: We learned how RNNs handle sequential data by incorporating recurrent connections to maintain information about past inputs.\n",
    "* LSTMs: We delved into LSTMs, a specialized type of RNN designed to address the vanishing gradient problem and capture long-term dependencies.\n",
    "* GANs: We explored the fascinating world of GANs, where two networks (generator and discriminator) compete to generate realistic data.\n",
    "\n",
    "We've not only covered the theory behind these architectures but also built practical examples using PyTorch, demonstrating their capabilities in image classification, text generation, time series prediction, and image generation.\n",
    "\n",
    "This workshop provides a solid foundation for further exploration of these architectures and their applications. You can delve deeper into each architecture, experiment with different datasets and tasks, and explore advanced topics like transfer learning, attention mechanisms, and reinforcement learning.\n",
    "\n",
    "The field of neural networks is constantly evolving, with new architectures and applications emerging rapidly. By understanding the fundamental principles and building hands-on experience, you'll be well-equipped to navigate this exciting landscape and contribute to the future of AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
